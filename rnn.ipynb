{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "In :numref:`sec_language_model` we introduced $n$-gram models, where the conditional probability of word $x_t$ at time step $t$ only depends on the $n-1$ previous words.\n",
    "If we want to incorporate the possible effect of words earlier than time step $t-(n-1)$ on $x_t$,\n",
    "we need to increase $n$.\n",
    "However, the number of model parameters would also increase exponentially with it, as we need to store $|\\mathcal{V}|^n$ numbers for a vocabulary set $\\mathcal{V}$.\n",
    "Hence, rather than modeling $P(x_t \\mid x_{t-1}, \\ldots, x_{t-n+1})$ it is preferable to use a latent variable model:\n",
    "\n",
    "$$P(x_t \\mid x_{t-1}, \\ldots, x_1) \\approx P(x_t \\mid h_{t-1}),$$\n",
    "\n",
    "where $h_{t-1}$ is a *hidden state* (also known as a hidden variable) that stores the sequence information up to time step $t-1$.\n",
    "In general,\n",
    "the hidden state at any time step $t$ could be computed based on both the current input $x_{t}$ and the previous hidden state $h_{t-1}$:\n",
    "\n",
    "$$h_t = f(x_{t}, h_{t-1}).$$\n",
    "\n",
    "For a sufficiently powerful function $f$ in :eqref:`eq_ht_xt`, the latent variable model is not an approximation. After all, $h_t$ may simply store all the data it has observed so far.\n",
    "However, it could potentially make both computation and storage expensive.\n",
    "\n",
    "Recall that we have discussed hidden layers with hidden units in :numref:`chap_perceptrons`.\n",
    "It is noteworthy that\n",
    "hidden layers and hidden states refer to two very different concepts.\n",
    "Hidden layers are, as explained, layers that are hidden from view on the path from input to output.\n",
    "Hidden states are technically speaking *inputs* to whatever we do at a given step,\n",
    "and they can only be computed by looking at data at previous time steps.\n",
    "\n",
    "*Recurrent neural networks* (RNNs) are neural networks with hidden states. Before introducing the RNN model, we first revisit the MLP model introduced in :numref:`sec_mlp`.\n",
    "\n",
    "## Neural Networks without Hidden States\n",
    "\n",
    "Let us take a look at an MLP with a single hidden layer.\n",
    "Let the hidden layer's activation function be $\\phi$.\n",
    "Given a minibatch of examples $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ with batch size $n$ and $d$ inputs, the hidden layer's output $\\mathbf{H} \\in \\mathbb{R}^{n \\times h}$ is calculated as\n",
    "\n",
    "$$\\mathbf{H} = \\phi(\\mathbf{X} \\mathbf{W}_{xh} + \\mathbf{b}_h).$$\n",
    ":eqlabel:`rnn_h_without_state`\n",
    "\n",
    "In :eqref:`rnn_h_without_state`, we have the weight parameter $\\mathbf{W}_{xh} \\in \\mathbb{R}^{d \\times h}$, the bias parameter $\\mathbf{b}_h \\in \\mathbb{R}^{1 \\times h}$, and the number of hidden units $h$, for the hidden layer.\n",
    "Thus, broadcasting (see :numref:`subsec_broadcasting`) is applied during the summation.\n",
    "Next, the hidden variable $\\mathbf{H}$ is used as the input of the output layer. The output layer is given by\n",
    "\n",
    "$$\\mathbf{O} = \\mathbf{H} \\mathbf{W}_{hq} + \\mathbf{b}_q,$$\n",
    "\n",
    "where $\\mathbf{O} \\in \\mathbb{R}^{n \\times q}$ is the output variable, $\\mathbf{W}_{hq} \\in \\mathbb{R}^{h \\times q}$ is the weight parameter, and $\\mathbf{b}_q \\in \\mathbb{R}^{1 \\times q}$ is the bias parameter of the output layer.  If it is a classification problem, we can use $\\text{softmax}(\\mathbf{O})$ to compute the probability distribution of the output categories.\n",
    "\n",
    "This is entirely analogous to the regression problem we solved previously in :numref:`sec_sequence`, hence we omit details.\n",
    "Suffice it to say that we can pick feature-label pairs at random and learn the parameters of our network via automatic differentiation and stochastic gradient descent.\n",
    "\n",
    "## Recurrent Neural Networks with Hidden States\n",
    ":label:`subsec_rnn_w_hidden_states`\n",
    "\n",
    "Matters are entirely different when we have hidden states. Let us look at the structure in some more detail.\n",
    "\n",
    "Assume that we have\n",
    "a minibatch of inputs\n",
    "$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$\n",
    "at time step $t$.\n",
    "In other words,\n",
    "for a minibatch of $n$ sequence examples,\n",
    "each row of $\\mathbf{X}_t$ corresponds to one example at time step $t$ from the sequence.\n",
    "Next,\n",
    "denote by $\\mathbf{H}_t  \\in \\mathbb{R}^{n \\times h}$ the hidden variable of time step $t$.\n",
    "Unlike the MLP, here we save the hidden variable $\\mathbf{H}_{t-1}$ from the previous time step and introduce a new weight parameter $\\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$ to describe how to use the hidden variable of the previous time step in the current time step. Specifically, the calculation of the hidden variable of the current time step is determined by the input of the current time step together with the hidden variable of the previous time step:\n",
    "\n",
    "$$\\mathbf{H}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh}  + \\mathbf{b}_h).$$\n",
    ":eqlabel:`rnn_h_with_state`\n",
    "\n",
    "Compared with :eqref:`rnn_h_without_state`, :eqref:`rnn_h_with_state` adds one more term $\\mathbf{H}_{t-1} \\mathbf{W}_{hh}$ and thus\n",
    "instantiates :eqref:`eq_ht_xt`.\n",
    "From the relationship between hidden variables $\\mathbf{H}_t$ and $\\mathbf{H}_{t-1}$ of adjacent time steps,\n",
    "we know that these variables captured and retained the sequence's historical information up to their current time step, just like the state or memory of the neural network's current time step. Therefore, such a hidden variable is called a *hidden state*.\n",
    "Since the hidden state uses the same definition of the previous time step in the current time step, the computation of :eqref:`rnn_h_with_state` is *recurrent*. Hence, neural networks with hidden states\n",
    "based on recurrent computation are named\n",
    "*recurrent neural networks*.\n",
    "Layers that perform\n",
    "the computation of :eqref:`rnn_h_with_state`\n",
    "in RNNs\n",
    "are called *recurrent layers*.\n",
    "\n",
    "\n",
    "There are many different ways for constructing RNNs.\n",
    "RNNs with a hidden state defined by :eqref:`rnn_h_with_state` are very common.\n",
    "For time step $t$,\n",
    "the output of the output layer is similar to the computation in the MLP:\n",
    "\n",
    "$$\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{hq} + \\mathbf{b}_q.$$\n",
    "\n",
    "Parameters of the RNN\n",
    "include the weights $\\mathbf{W}_{xh} \\in \\mathbb{R}^{d \\times h}, \\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$,\n",
    "and the bias $\\mathbf{b}_h \\in \\mathbb{R}^{1 \\times h}$\n",
    "of the hidden layer,\n",
    "together with the weights $\\mathbf{W}_{hq} \\in \\mathbb{R}^{h \\times q}$\n",
    "and the bias $\\mathbf{b}_q \\in \\mathbb{R}^{1 \\times q}$\n",
    "of the output layer.\n",
    "It is worth mentioning that\n",
    "even at different time steps,\n",
    "RNNs always use these model parameters.\n",
    "Therefore, the parameterization cost of an RNN\n",
    "does not grow as the number of time steps increases.\n",
    "\n",
    ":numref:`fig_rnn` illustrates the computational logic of an RNN at three adjacent time steps.\n",
    "At any time step $t$,\n",
    "the computation of the hidden state can be treated as:\n",
    "(i) concatenating the input $\\mathbf{X}_t$ at the current time step $t$ and the hidden state $\\mathbf{H}_{t-1}$ at the previous time step $t-1$;\n",
    "(ii) feeding the concatenation result into a fully-connected layer with the activation function $\\phi$.\n",
    "The output of such a fully-connected layer is the hidden state $\\mathbf{H}_t$ of the current time step $t$.\n",
    "In this case,\n",
    "the model parameters are the concatenation of $\\mathbf{W}_{xh}$ and $\\mathbf{W}_{hh}$, and a bias of $\\mathbf{b}_h$, all from :eqref:`rnn_h_with_state`.\n",
    "The hidden state of the current time step $t$, $\\mathbf{H}_t$, will participate in computing the hidden state $\\mathbf{H}_{t+1}$ of the next time step $t+1$.\n",
    "What is more, $\\mathbf{H}_t$ will also be\n",
    "fed into the fully-connected output layer\n",
    "to compute the output\n",
    "$\\mathbf{O}_t$ of the current time step $t$.\n",
    "\n",
    "![An RNN with a hidden state.](http://d2l.ai/_images/rnn.svg)\n",
    ":label:`fig_rnn`\n",
    "\n",
    "We just mentioned that the calculation of $\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh}$ for the hidden state is equivalent to\n",
    "matrix multiplication of\n",
    "concatenation of $\\mathbf{X}_t$ and $\\mathbf{H}_{t-1}$\n",
    "and\n",
    "concatenation of $\\mathbf{W}_{xh}$ and $\\mathbf{W}_{hh}$.\n",
    "Though this can be proven in mathematics,\n",
    "in the following we just use a simple code snippet to show this.\n",
    "To begin with,\n",
    "we define matrices `X`, `W_xh`, `H`, and `W_hh`, whose shapes are (3, 1), (1, 4), (3, 4), and (4, 4), respectively.\n",
    "Multiplying `X` by `W_xh`, and `H` by `W_hh`, respectively, and then adding these two multiplications,\n",
    "we obtain a matrix of shape (3, 4).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: What is perplexity?\n",
    "\n",
    "https://towardsdatascience.com/perplexity-in-language-models-87a196019a94"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "* A neural network that uses recurrent computation for hidden states is called a recurrent neural network (RNN).\n",
    "* The hidden state of an RNN can capture historical information of the sequence up to the current time step.\n",
    "* The number of RNN model parameters does not grow as the number of time steps increases.\n",
    "* We can create character-level language models using an RNN.\n",
    "* We can use perplexity to evaluate the quality of language models.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. If we use an RNN to predict the next character in a text sequence, what is the required dimension for any output?\n",
    "   \n",
    "    **A**: Any output must have dimension 1. $O \\in \\R ^{1 \\times 1}$\n",
    "\n",
    "2. Why can RNNs express the conditional probability of a token at some time step based on all the previous tokens in the text sequence?\n",
    "   \n",
    "   **A**: The information of all the previous tokens are encapsulated in what is called the hidden state. With this information, a probability of the next token being some value can be generated.\n",
    "\n",
    "3. What happens to the gradient if you backpropagate through a long sequence?\n",
    "\n",
    "    **A**: There might be the issue of vanishing or exploding gradients.\n",
    "\n",
    "4. What are some of the problems associated with the language model described in this section?\n",
    "\n",
    "    **A**: Character-Level language models are unlikely to learn meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Recurrent Neural Networks from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%matplotlib inline\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from ipynb.fs.full.text_models import load_data_time_machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import try_gpu\n",
    "from utils import Accumulator, Timer, sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise to demonstrate one-hot encoding and transposition so that the tensor has dimensions (nubmer of time steps, batch_size, vocabulary size). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor([0, 2]), len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(10).reshape((2, 5))\n",
    "F.one_hot(X.T, 28).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the rnn class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(vocab_size, num_hiddens, device):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return torch.randn(size=shape, device=device) * 0.01\n",
    "\n",
    "    # Hidden layer parameters\n",
    "    W_xh = normal((num_inputs, num_hiddens))\n",
    "    W_hh = normal((num_hiddens, num_hiddens))\n",
    "    b_h = torch.zeros(num_hiddens, device=device)\n",
    "    # Output layer parameters\n",
    "    W_hq = normal((num_hiddens, num_outputs))\n",
    "    b_q = torch.zeros(num_outputs, device=device)\n",
    "    # Attach gradients\n",
    "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_rnn_state(batch_size, num_hiddens, device):\n",
    "    # Returns the hidden state at initialisation\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device), )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following rnn function defines how to compute the hidden state and output at a time step. Note that the RNN model loops through the outermost dimension of inputs so that it updates hidden states H of a minibatch, time step by time step. Besides, the activation function here uses the  function. As described in Section 4.1, the mean value of the  function is 0, when the elements are uniformly distributed over the real numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(inputs, state, params):\n",
    "    \"\"\"defines how to compute the hidden state and output at a time step\n",
    "\n",
    "    Args:\n",
    "        inputs (tensor): collection of features\n",
    "        state (tuple): hidden state, tuple of a tensor\n",
    "        params (list): list of model parameters, which are tensors\n",
    "\n",
    "    Returns:\n",
    "        outputs (tensor), hidden state (tuple)\n",
    "    \"\"\"\n",
    "    # Here `inputs` shape: (`num_steps`, `batch_size`, `vocab_size`)\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    # Shape of `X`: (`batch_size`, `vocab_size`)\n",
    "    for X in inputs:\n",
    "        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)\n",
    "        Y = torch.mm(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return torch.cat(outputs, dim=0), (H,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the needed functions being defined, next we create a class to wrap these functions and store parameters for an RNN model implemented from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModelScratch: \n",
    "    \"\"\"A RNN Model implemented from scratch.\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, device,\n",
    "                 get_params, init_state, forward_fn):\n",
    "        \"\"\"Initialisation of RNN\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): size of the vocabulary\n",
    "            num_hiddens (int): number of hidden units\n",
    "            device (torch.device): 'cpu' or 'cuda', where the tensor will be located\n",
    "            get_params (func): initialiser for RNN parameters\n",
    "            init_state (func): defines initial hidden state\n",
    "            forward_fn (func): function that steps forward in RNN\n",
    "        \"\"\"\n",
    "        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n",
    "        self.params = get_params(vocab_size, num_hiddens, device)\n",
    "        self.init_state, self.forward_fn = init_state, forward_fn\n",
    "\n",
    "    def __call__(self, X, state):\n",
    "        \"\"\"steps forward\"\"\"\n",
    "        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
    "        return self.forward_fn(X, state, self.params)\n",
    "\n",
    "    def begin_state(self, batch_size, device):\n",
    "        \"\"\"initialisation \"\"\"\n",
    "        return self.init_state(batch_size, self.num_hiddens, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 28]), 1, torch.Size([2, 512]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hiddens = 512\n",
    "net = RNNModelScratch(len(vocab), num_hiddens, try_gpu(), get_params,\n",
    "                      init_rnn_state, rnn)\n",
    "state = net.begin_state(X.shape[0], try_gpu())\n",
    "Y, new_state = net(X.to(try_gpu()), state)\n",
    "Y.shape, len(new_state), new_state[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the output shape is (number of time steps x batch size, vocabulary size), while the hidden state shape remains the same, i.e., (batch size, number of hidden units)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first define the prediction function to generate new characters following the user-provided prefix, which is a string containing several characters. When looping through these beginning characters in prefix, we keep passing the hidden state to the next time step without generating any output. This is called the warm-up period, during which the model updates itself (e.g., update the hidden state) but does not make predictions. After the warm-up period, the hidden state is generally better than its initialized value at the beginning. So we generate the predicted characters and emit them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(prefix, num_preds, net, vocab, device):  \n",
    "    \"\"\"Generate new characters following the `prefix`.\"\"\"\n",
    "    state = net.begin_state(batch_size=1, device=device)\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n",
    "\n",
    "    for y in prefix[1:]:  # Warm-up period\n",
    "        _, state = net(get_input(), state)\n",
    "        outputs.append(vocab[y])\n",
    "\n",
    "    for _ in range(num_preds):  # Predict `num_preds` steps\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the predict_ch8 function. We specify the prefix as time traveller and have it generate 10 additional characters. Given that we have not trained the network, it will generate nonsensical predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time traveller hlrc<unk>pldqo'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('time traveller ', 10, net, vocab, try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping\n",
    "Method to alleviate the problem of exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(net, theta):  #@save\n",
    "    \"\"\"Clip the gradient.\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the model, let us define a function to train the model in one epoch. It differs from how we train the model of Section 3.6 in three places:\n",
    "\n",
    "Different sampling methods for sequential data (random sampling and sequential partitioning) will result in differences in the initialization of hidden states.\n",
    "\n",
    "We clip the gradients before updating the model parameters. This ensures that the model does not diverge even when gradients blow up at some point during the training process.\n",
    "\n",
    "We use perplexity to evaluate the model. As discussed in Section 8.4.4, this ensures that sequences of different length are comparable.\n",
    "\n",
    "Specifically, when sequential partitioning is used, we initialize the hidden state only at the beginning of each epoch. Since the  subsequence example in the next minibatch is adjacent to the current  subsequence example, the hidden state at the end of the current minibatch will be used to initialize the hidden state at the beginning of the next minibatch. In this way, historical information of the sequence stored in the hidden state might flow over adjacent subsequences within an epoch. However, the computation of the hidden state at any point depends on all the previous minibatches in the same epoch, which complicates the gradient computation. To reduce computational cost, we detach the gradient before processing any minibatch so that the gradient computation of the hidden state is always limited to the time steps in one minibatch.\n",
    "\n",
    "When using the random sampling, we need to re-initialize the hidden state for each iteration since each example is sampled with a random position. Same as the train_epoch_ch3 function in Section 3.6, updater is a general function to update the model parameters. It can be either the d2l.sgd function implemented from scratch or the built-in optimization function in a deep learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net, train_iter, loss, updater, device, use_random_iter):\n",
    "    \"\"\"Train a net within one epoch (defined in Chapter 8).\"\"\"\n",
    "    state, timer = None, Timer()\n",
    "    metric = Accumulator(2)  # Sum of training loss, no. of tokens\n",
    "    for X, Y in train_iter:\n",
    "        if state is None or use_random_iter:\n",
    "            # Initialize `state` when either it is the first iteration or\n",
    "            # using random sampling\n",
    "            state = net.begin_state(batch_size=X.shape[0], device=device)\n",
    "        else:\n",
    "            if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
    "                # `state` is a tensor for `nn.GRU`\n",
    "                state.detach_()\n",
    "            else:\n",
    "                # `state` is a tuple of tensors for `nn.LSTM` and\n",
    "                # for our custom scratch implementation\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "        y = Y.T.reshape(-1)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_hat, state = net(X, state)\n",
    "        l = loss(y_hat, y.long()).mean()\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            updater.zero_grad()\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            updater.step()\n",
    "        else:\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            # Since the `mean` function has been invoked\n",
    "            updater(batch_size=1)\n",
    "        metric.add(l * y.numel(), y.numel())\n",
    "    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, vocab, lr, num_epochs, device,\n",
    "              use_random_iter=False):\n",
    "    \"\"\"Train a model (defined in Chapter 8).\"\"\"\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    #animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',\n",
    "    #                        legend=['train'], xlim=[10, num_epochs])\n",
    "    # Initialize\n",
    "    if isinstance(net, nn.Module):\n",
    "        updater = torch.optim.SGD(net.parameters(), lr)\n",
    "    else:\n",
    "        updater = lambda batch_size: sgd(net.params, lr, batch_size)\n",
    "#: predict(prefix, 50, net, vocab, device)\n",
    "    predict_new = lambda prefix: predict(prefix, 50, net, vocab, device)\n",
    "    # Train and predict\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl, speed = train_epoch(\n",
    "            net, train_iter, loss, updater, device, use_random_iter)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(predict_new('time traveller')) #animator.add(epoch + 1, [ppl])\n",
    "            \n",
    "    print(f'perplexity {ppl:.1f}, {speed:.1f} tokens/sec on {str(device)}')\n",
    "    print(predict_new('time traveller'))\n",
    "    print(predict_new('traveller'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time traveller and the the thing the thing the thing the thing t\n",
      "time travellere the and he pare the and he pare the and he pare \n",
      "time traveller and the the the the the the the the the the the t\n",
      "time traveller the this the this the this the this the this the \n",
      "time traveller and the this the this that in thave the this the \n",
      "time traveller at in the the the the the the the the the the the\n",
      "time travellereace the that the that the that the that the that \n",
      "time traveller thace dimensions and the time traveller and this \n",
      "time traveller all and hevere ard ancthere and her and and and a\n",
      "time traveller all and heriget to vericelingthit thavenethis tha\n",
      "time traveller but so hine sime sion sime traveller and sine an \n",
      "time traveller to kens have thee the ghas expain the medice veri\n",
      "time traveller but seat in ti becentrone s ancalle aldot scithth\n",
      "time traveller ofres you ir the stallexpoche fire wis han seat i\n",
      "time traveller but now hay fige sati has inve lo than an ang the\n",
      "time traveller ffrenes of at innas if on whe dimension dite mith\n",
      "time traveller prodend and in aimetalien at is that hesthas nots\n",
      "time traveller trine of this this is th plyprace thing hthe we c\n",
      "time traveller tarkeerofin sinecaid thi ghint and whed and as it\n",
      "time traveller brtcended y is fou be in inof dth frumatherarical\n",
      "time travellerit s against reason said the time travellerit s ag\n",
      "time traveller froc the packing ag the matila sllor athe time tr\n",
      "time travellerit s against reason said fflly of ugarnect and ine\n",
      "time travellerit s against reason said filby an covera oun lis t\n",
      "time traveller for so it wiscoulen an assidilather think bes abl\n",
      "time travellerit s against reason said filby an thaura ta ie tom\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller arter tounted foc pou dring and tomise aum ani te\n",
      "time traveller for so it will be core sadeif ou rad te bat le ha\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time travellerit s against reason said filbywhat waid a for one \n",
      "time traveller after the pauserequired for the proper assimilati\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time traveller wotcradow ar cuthed he the guttle gethen bow y ac\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller with a slight accession ofcheerfulness really thi\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time travellerywi caid the tell sobut you in to eo sicbubread i \n",
      "time traveller with a slight accession ofcheerfulness really thi\n",
      "perplexity 1.0, 94066.2 tokens/sec on cpu\n",
      "time traveller with a slight accession ofcheerfulness really thi\n",
      "traveller with a slight accession ofcheerfulness really thi\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 500, 1\n",
    "train(net, train_iter, vocab, lr, num_epochs, try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time travellere the the the the the the the the the the the the \n",
      "time travellere the the the the the the the the the the the the \n",
      "time traveller the the the the the the the the the the the the t\n",
      "time travellere the the the the the the the the the the the the \n",
      "time travellere the the the the the the the the the the the the \n",
      "time traveller and the the the the the the the the the the the t\n",
      "time traveller and and the the the the the the the the the the t\n",
      "time travellere the the the the the the the the the the the the \n",
      "time travellere the the the the the the the the the the the the \n",
      "time traveller and the the the the the the the the the the the t\n",
      "time traveller and the the the the the the the the the the the t\n",
      "time travellere the thise as in the thise as in the thise as in \n",
      "time traveller and the thise dimensions of the thate the thise d\n",
      "time traveller the time traveller and the time traveller the thi\n",
      "time traveller the mane the this the redithe mane the thee there\n",
      "time traveller of the then in the mine sime sime sime sime sime \n",
      "time traveller out nt of the this the redine sion and his the me\n",
      "time traveller care all har are it the mile said the time thavel\n",
      "time traveller of than in that is cean this as in another tare a\n",
      "time traveller howhin we to the on the onge traceext move about \n",
      "time traveller cald an mure cannots a caine so the lbyer carnell\n",
      "time travellerit sof land have all stare dif the than thing this\n",
      "time travellerit s aglint suig the time traveller hescuncline so\n",
      "time traveller hard and hivetifele in in the file isterenglly yo\n",
      "time travellerit would bet towell ghougare a fich urimintistlo i\n",
      "time traveller helenit that is that is meat fice ders a dithy ur\n",
      "time travellerit s against reason said the time travellerit s a \n",
      "time travellerit s against reason said filby an argumentative pe\n",
      "time travellerit s agonit ter fis this in thedother whthree that\n",
      "time travellerit s against reason said the medical man there are\n",
      "time traveller hald is on a fian which has only tho discover a s\n",
      "time travellerit s against reason said filbycan a cube that does\n",
      "time travellerit soughive be a ghat aly tomer and persis tantsei\n",
      "time traveller and the gatme to s atter the prover anse is his t\n",
      "time travellerit s agaid tha tile thithing the time travellerit \n",
      "time traveller after disne have been asking why three dimensions\n",
      "time traveller smiled tor coul inothe fire iftime is reay i onve\n",
      "time traveller ait th y manll of at gimmedte ar fackess and wher\n",
      "time traveller sumention of this know very well thattime is only\n",
      "time traveller held in his hand was a glitteringmetallin frane o\n",
      "time travellerit s against reason cal ot sthat is just wherethe \n",
      "time traveller held in his hand so me mavesbell in the fisterits\n",
      "time travellerit s against reason said filby an argumentative pe\n",
      "time travellerit would be remarkable thing he took one of the sm\n",
      "time traveller held in his hand was a glitteringmetallic framewo\n",
      "time travellerit s against reason said filbycan a cube that does\n",
      "time travellerit s against reason said filby an argumentative pe\n",
      "time travellerit s against reason said filbycan a cube that does\n",
      "time travellerit s against reason said filby sace alliry some ti\n",
      "time traveller proceeded anyreal body must have extension in fou\n",
      "perplexity 1.5, 88024.2 tokens/sec on cpu\n",
      "time traveller proceeded anyreal body must have extension in fou\n",
      "traveller after the pauserequired for the proper assimilati\n"
     ]
    }
   ],
   "source": [
    "net = RNNModelScratch(len(vocab), num_hiddens, try_gpu(), get_params,\n",
    "                      init_rnn_state, rnn)\n",
    "train(net, train_iter, vocab, lr, num_epochs, try_gpu(),\n",
    "          use_random_iter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "* We can train an RNN-based character-level language model to generate text following the user-provided text prefix.\n",
    "* A simple RNN language model consists of input encoding, RNN modeling, and output generation.\n",
    "* RNN models need state initialization for training, though random sampling and sequential partitioning use different ways.\n",
    "* When using sequential partitioning, we need to detach the gradient to reduce computational cost.\n",
    "* A warm-up period allows a model to update itself (e.g., obtain a better hidden state than its initialized value) before making any prediction.\n",
    "* Gradient clipping prevents gradient explosion, but it cannot fix vanishing gradients.\n",
    "\n",
    "## Exercises\n",
    "1. Show that one-hot encoding is equivalent to picking a different embedding for each object.\n",
    "   \n",
    "   **A**: Embedding is a mapping of a discrete variable to a vector of contnuous numbers (https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526). If each object has a different embedding, then the number of unique values the vector can take equals the number of unique objects, which is basically one-hot encoding.\n",
    "\n",
    "2. Adjust the hyperparameters (e.g., number of epochs, number of hidden units, number of time steps in a minibatch, and learning rate) to improve the perplexity.\n",
    "   1. How low can you go?\n",
    "   2. Replace one-hot encoding with learnable embeddings. Does this lead to better performance?\n",
    "   3. How well will it work on other books by H. G. Wells, e.g., *The War of the Worlds*?\n",
    "3. Modify the prediction function such as to use sampling rather than picking the most likely next character\n",
    "   1. What happens?\n",
    "   2. Bias the model towards more likely outputs, e.g., by sampling from  for .\n",
    "4. Run the code in this section without clipping the gradient. What happens?\n",
    "5. Change sequential partitioning so that it does not separate hidden states from the computational graph. Does the running time change? How about the perplexity?\n",
    "6. Replace the activation function used in this section with ReLU and repeat the experiments in this section. Do we still need gradient clipping? Why?"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de75f5aef4715fdb9e70b2d0e4ac5274329e92ec7eafb85fdd5cc4f6c7320198"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('data-science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
